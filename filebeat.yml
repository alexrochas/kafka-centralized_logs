# filebeat.yml

filebeat.prospectors:
- type: log
  json.keys_under_root: true
# Json key name, which value contains a sub JSON document produced by our application Console Appender
  json.message_key: log
  enabled: true
  encoding: utf-8
  document_type: docker
  paths:
# Location of all our Docker log files (mapped volume in docker-compose.yml)
    - '/usr/share/filebeat/dockerlogs/data/*/*.log'
processors:
# decode the log field (sub JSON document) if JSONencoded, then maps it's fields to elasticsearch fields
- decode_json_fields:
    fields: ["log"]
    target: ""
# overwrite existing target elasticsearch fields while decoding json fields
    overwrite_keys: true
- add_docker_metadata: ~

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

setup.template.settings:
  index.number_of_shards: 3

#setup.kibana:
# if your kibana is local you can change it to 127.0.0.1:80
#  host: "127.0.0.1:80"
#  protocol: "http"
# identification required for X-pack
#  username: "my_login"
#  password: "my_password"
# path needed if kibana not at website root
#  path: "/kibana"

#output.elasticsearch:
# if your elasticsearch is local you can change it to 127.0.0.1:9200
#  hosts: ["localhost:9200"]
#  template:
#    name: "filebeat"
#    path: "fields.yml"
#    overwrite: true
#  protocol: "http"
# identification required for X-pack
#  username: "elastic"
#  password: "changeme"


output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["localhost:9092"]
  # message topic selection + partitioning
  topic: '%{[log_topic]:dummy}-log'
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000

# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.to_files: true
logging.to_syslog: false

# X-pack optional module
#xpack.monitoring.enabled: true
#xpack.monitoring.elasticsearch:
